{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hate Speech Detection on Tweets\n",
    "## 蕭云雅口無言不由衷於寫出來了\n",
    "### **- 陳冠穎 / 統計109 / H24051312<br />- 李艾霓 / 統計111 / H24076095**\n",
    "#### 2019/5/17\n",
    "* **1 競賽敘述與目標**\n",
    "    * 1.1 問題分析\n",
    "    * 1.2 作法簡述\n",
    "* **2 資料前處理**\n",
    "    * 2.1 載入資料\n",
    "    * 2.2 清理tweets\n",
    "* **3 文字轉換為矩陣**\n",
    "    * 3.1 CountVectorizer\n",
    "* **4 建構模型**\n",
    "    * 4.1 Train Test Split\n",
    "    * 4.2 訓練模型\n",
    "    * 4.3 將結果合併\n",
    "    * 4.4 最終預測結果及輸出\n",
    "* **5 預測結果分析**\n",
    "    * 5.1 驗證資料得分\n",
    "    * 5.2 測試資料上傳得分\n",
    "    * 5.3 結果分析\n",
    "* **6 補充**\n",
    "* **7 心得與感想**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 競賽敘述與目標\n",
    "\n",
    "　　此競賽為2019年國立成功大學統計學系李政德老師所開設的選修課程「資料科學導論」中之小組競賽(第二次)，在此競賽中我們會獲得兩個資料集，分別為訓練資料(14869筆)及測試資料(9914筆)，而我們的任務則是透過tweet的內文將其正確分成三類：「0=hateful, 1=offensive, 2=clean」(即訓練資料中的\"class\"欄位)，最終將我們預測測試資料的結果上傳到課程[競賽網站](http://140.116.52.202:5566/)，即獲得預測分數。\n",
    "- 分數的計算方式為: 0.6\\*HateF + 0.4\\*AllF\n",
    "\n",
    "### 1.1 問題分析\n",
    "　　在這個競賽中要做的是一個多類別的文字分類問題，而分析三個類別：Hateful, Offensive, Clean，不難發現三者之間其實有著程度上的差異，一個有趣的問題是：在這三者之間的界線我們該如何界定？\n",
    "\n",
    "　　對於同樣的一個句子，每個人的感受必定會有所不同，舉例來說：\"yall niggas b cuffing hoes cause yall aint never have bitches\" 和 \"@nosfiend215 @faggot696969 niggers\"這兩則tweet，你會分別將他們歸為哪一類？這大概不是那麼容易可以分得出來的。\n",
    "\n",
    "　　對於這樣的問題，真的有標準答案嗎？\n",
    "\n",
    "　　在這個資料集中，我們並不知道這些答案是如何被決定出來的，更不能夠確定這些文字在被人工加上標籤時，他們的標準是否維持一致(尤其我想對於這種帶有主觀意識的問題，這想必是很難做到的)，若這之間並不存在一個標準，那機器當然也會很難學到一個規則來做出正確的預測，使結果難以達到預期。\n",
    "   \n",
    "　　不過撇除上面的議題，我們今天在競賽中所關注的部分僅止於我們的模型是否能夠對於一個句子做出和資料集相同的預測，所以與其說我們要讓機器學習如何分類出文字的語意及情緒，或許說我們要讓機器學習到這個資料集中分類的一些規則會來得更加精確。\n",
    "   \n",
    "### 1.2 作法簡述\n",
    "　　這份Note book只包含了我們在競賽中最後上傳結果所出自的那一個模型，各個分類器中的參數都還欠缺調整，必須說這並不是最好的一個模型。另外，我們在競賽結束之後還有嘗試使用了bert，不過這個部分就放到最後再做補充。\n",
    "  \n",
    "　　首先我們會先簡單處理資料，將**@tag**以及**http://網址**去除，亦會將多個連續的空白符縮減為一個。接著將文字做詞幹提取以及詞形還原，同時將其轉為詞頻矩陣，轉換完成的資料即準備好可以餵給模型了。這裡總共使用了三個不同的模型：SVC、XGBC以及LR。我們將三個分類器預測的結果以較特殊的規則合併，產生最終預測。\n",
    "  \n",
    "　　礙於時間不足，這三個模型並**沒有**被最佳化，其中XGBC的參數是隨意設置的，而LR則是完全使用預設參數，若想提升預測結果，可以考慮使用GridSearchCV重新調整參數，亦可以令各分類器預測出機率，再人工調整分類的界線，使分類結果更優(例如在我們的競賽中，HateF佔了較高的分數，我們可以降低分類為0的標準來使recall提高，同時會提高F1，就能得到比較好的分數)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in our libraries\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "from wordcloud import WordCloud,STOPWORDS\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, ENGLISH_STOP_WORDS\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import f1_score, roc_auc_score, classification_report\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "import re\n",
    "#nltk.download('stopwords') \n",
    "from nltk.corpus import stopwords\n",
    "import pickle \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk import word_tokenize\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 資料前處理\n",
    "### 2.1 載入資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4394</th>\n",
       "      <td>1</td>\n",
       "      <td>RT @HelloCupkake: Too many good single girls, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>799</th>\n",
       "      <td>1</td>\n",
       "      <td>@New_Dinero Alright pussy.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      class                                              tweet\n",
       "4394      1  RT @HelloCupkake: Too many good single girls, ...\n",
       "799       1                         @New_Dinero Alright pussy."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv(\"train.csv\")\n",
    "test = pd.read_csv(\"test.csv\")\n",
    "\n",
    "train.sample(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((14869, 2), (9914, 2))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 清理tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>tweet</th>\n",
       "      <th>cleaned_tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>[9-1-13] 2:50 pm \"son of a bitch ate my mac n ...</td>\n",
       "      <td>[9-1-13] 2:50 pm \"son of a bitch ate my mac n ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>RT @BryceSerna: Don't be a pussy grab the boot...</td>\n",
       "      <td>RT : Don't be a pussy grab the booty. Love the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>RT @ClicquotSuave: bunch of rappers boutta flo...</td>\n",
       "      <td>RT : bunch of rappers boutta flood the interne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>@michigannews13 wow. Thats great language comi...</td>\n",
       "      <td>wow. Thats great language coming from a HS co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>and this is why I'm single, I don't fuck with ...</td>\n",
       "      <td>and this is why I'm single, I don't fuck with ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   class                                              tweet  \\\n",
       "0      1  [9-1-13] 2:50 pm \"son of a bitch ate my mac n ...   \n",
       "1      1  RT @BryceSerna: Don't be a pussy grab the boot...   \n",
       "2      2  RT @ClicquotSuave: bunch of rappers boutta flo...   \n",
       "3      2  @michigannews13 wow. Thats great language comi...   \n",
       "4      1  and this is why I'm single, I don't fuck with ...   \n",
       "\n",
       "                                       cleaned_tweet  \n",
       "0  [9-1-13] 2:50 pm \"son of a bitch ate my mac n ...  \n",
       "1  RT : Don't be a pussy grab the booty. Love the...  \n",
       "2  RT : bunch of rappers boutta flood the interne...  \n",
       "3   wow. Thats great language coming from a HS co...  \n",
       "4  and this is why I'm single, I don't fuck with ...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer = WordNetLemmatizer()\n",
    "for X in [train,test]:\n",
    "    documents = []\n",
    "    for i in range(0, len(X)):\n",
    "        \n",
    "        # Remove @user & http://...\n",
    "        document = re.sub(r\"(@[A-Za-z0-9]+)|(https?://[A-Za-z0-9./]+)\", \" \", str(X.at[i,'tweet']))\n",
    "\n",
    "        # Substituting multiple spaces with single space\n",
    "        document = re.sub(r'\\s+', ' ', document, flags=re.I)\n",
    "\n",
    "        documents.append(document)\n",
    "    X['cleaned_tweet'] = documents\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 文字轉換為矩陣\n",
    "### 3.1 CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LemmaTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "    def __call__(self, doc):\n",
    "        return [self.wnl.lemmatize(t) for t in word_tokenize(doc)]\n",
    "\n",
    "stemmer = SnowballStemmer(\"english\", ignore_stopwords=True)\n",
    "\n",
    "#In order to trasform text data into vectors\n",
    "#We combine the Stemmer with sklearn-Countvectorizer\n",
    "class StemmedCountVectorizer(CountVectorizer):\n",
    "    def build_analyzer(self):\n",
    "        analyzer = super(StemmedCountVectorizer, self).build_analyzer()\n",
    "        return lambda doc: ([stemmer.stem(w) for w in analyzer(doc)])\n",
    "    \n",
    "vect = StemmedCountVectorizer(min_df= 1,ngram_range=(1,3),tokenizer = LemmaTokenizer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 建構模型\n",
    "\n",
    "### 4.1 Train Test Split\n",
    "　　在**建立Model的步驟**使用第一個cell做train_test_split; 要**做test.csv的預測**用第二個cell來取得測試集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(train['cleaned_tweet'],train['class'],test_size=0.3,random_state=333)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test.csv Predict\n",
    "\n",
    "X_train = train['cleaned_tweet']\n",
    "y_train = train['class']\n",
    "X_test = test['cleaned_tweet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect.fit(X_train)\n",
    "X_train_vectorized = vect.transform(X_train)\n",
    "X_test_vectorized = vect.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 訓練模型\n",
    "　　若是建構模型的階段，將最後一行取消註解可以看到預測結果的F1_score。\n",
    "#### 1. SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 : 0.7156383968342981\n"
     ]
    }
   ],
   "source": [
    "model1 = SVC(kernel='linear',class_weight={0:7,1:1.5,2:1.5},probability=True).fit(X_train_vectorized, y_train)\n",
    "pred1 = model1.predict(X_test_vectorized)\n",
    "#print('F1 :', f1_score(y_test, pred1,average='macro')) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "　　為了使結果更佳，我們改為預測機率再對結果做調整。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 : 0.7400942306689897\n"
     ]
    }
   ],
   "source": [
    "test_pred = model1.predict_proba(X_test_vectorized)\n",
    "pred1 = []\n",
    "for i in range(len(test_pred)):\n",
    "    if test_pred[i,0]>0.1565 or (test_pred[i,0]>test_pred[i,1] and test_pred[i,0]>test_pred[i,2]):\n",
    "        pred1.append(0)\n",
    "    elif test_pred[i,2]>0.353 or test_pred[i,2]>test_pred[i,1]:\n",
    "        pred1.append(2)\n",
    "    else:\n",
    "        pred1.append(1)\n",
    "#print('F1 :', f1_score(y_test, pred1,average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 : 0.7063962360560193\n"
     ]
    }
   ],
   "source": [
    "model2 = XGBClassifier(subsample=0.6,max_depth=13).fit(X_train_vectorized, y_train)\n",
    "pred2 = model2.predict(X_test_vectorized)\n",
    "#print('F1 :', f1_score(y_test, pred2,average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. LogisticRrgression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 : 0.6650381990211757\n"
     ]
    }
   ],
   "source": [
    "model3 = LogisticRegression().fit(X_train_vectorized, y_train)\n",
    "pred3 = model3.predict(X_test_vectorized)\n",
    "#print('F1 :', f1_score(y_test, pred3,average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 將結果合併"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 : 0.7468469830338492\n"
     ]
    }
   ],
   "source": [
    "pred = []\n",
    "for i in range(len(pred1)):\n",
    "    p = [pred1[i],pred2[i],pred3[i]]\n",
    "\n",
    "    if p.count(0)>0:\n",
    "        pred.append(0)\n",
    "    elif p.count(2)>1:\n",
    "        pred.append(2)\n",
    "    elif p.count(1)>1:\n",
    "        pred.append(1)\n",
    "    else:\n",
    "        pred.append(p[1])\n",
    "y_test=np.array(y_test)\n",
    "#print('F1 :', f1_score(y_test, pred,average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 最終預測及輸出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(data = {'id' : test.id, 'class' : pred})\n",
    "results.to_csv('results.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 預測結果及分析\n",
    "### 5.1 驗證資料得分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Confusion = \n",
      "[[ 130  133   30]\n",
      " [ 127 3164   80]\n",
      " [  19  108  670]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.44      0.47      0.46       276\n",
      "           1       0.94      0.93      0.93      3405\n",
      "           2       0.84      0.86      0.85       780\n",
      "\n",
      "   micro avg       0.89      0.89      0.89      4461\n",
      "   macro avg       0.74      0.75      0.75      4461\n",
      "weighted avg       0.89      0.89      0.89      4461\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#print('Test Confusion = \\n{}'.format(metrics.confusion_matrix(pred, y_test)))\n",
    "#print(classification_report(y_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 測試資料上傳得分\n",
    "#### Public Set\n",
    "<img src=\"img/public.png\" alt=\"PublicSet\" />\n",
    "\n",
    "#### Private Set\n",
    "<img src=\"img/private.jpg\" alt=\"PrvateSet\" />\n",
    "\n",
    "### 5.3 結果分析\n",
    "　　上傳結果HateF的項目不太確定詳細的算法，但顯然是和我們用的算法不一樣，AllF倒是沒有太大的差距，我們的驗證集的結果應該是和上傳得分沒有太大差距的。而在競賽中為了不讓Public和Private的分數有差距，我們堅持不針對上傳結果回來調整模型，而是完全以驗證資料為基準來校正，我想這是使我們最終在Private Set上排名沒有倒退很重要的一點。\n",
    "  \n",
    "## 6. 補充\n",
    "　　其實在這中間我們還花了不少的時間在研究**BERT**，不過很可惜的是最終我們還是沒有找到合適的機器來跑，也因此浪費了很多時間，最後才趕著在截止之前回來做這些比較簡單的模型，因為時間的不足而在競賽上沒有滿意的結果。\n",
    "  \n",
    "　　雖然來不及在競賽結束之前做完，但是在結束後我們還是想辦法跑出結果了:)\n",
    "  \n",
    "　　畢竟是做了很久的東西，還是會很好奇到底能得到怎樣的分數，在這裡也分享一下最後的結果～\n",
    "  \n",
    "<img src=\"img/bert.jpg\" alt=\"bertResult\" width = \"400\" />\n",
    "\n",
    "　　模型預測出來的是Probability，上半部是用np.argmax()直接轉換成分類得到的結果，而下半部則是和我們在這裡的模型一樣，調整過分類界線所得到的結果。其實並沒有我們預先所想像的那麼神奇xD，不過F1能有0.3的進步確實也是不小了，和競賽的第一名相比分數也是高出了一些，尤其這個結果是來自於完全沒有經過處理的資料，如果在把資料餵進去之前先做初步的處理說不定還能夠有更好的結果～(不過train一次實在要太久了，我們就來不及再跑一次了哈哈)\n",
    "  \n",
    "## 7. 心得與感想"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/kevin.jpg\" width = \"180\" height = \"180\" alt=\"MD小穎\" align=left />\n",
    "\n",
    "### 小穎\n",
    "　　這次的主題是關於文字處理方面，一開始覺得很新鮮，因為自己本身對於這方面也陌生很多，也在想，如果是文字的話就很難像數字一樣量化，或是有倍率關係，一開始想了很多方法，把一些重複字刪掉啊，刪除奇怪的符號、暱稱等等，找了很多處理文字的模型什麼的，再套用入傳統的一些模型，利用logistic regression之類的，不過其實結果都大同小異，也因為最近很多期中考和作業的關係，所以其實付出的時間比上次少很多，不過我發現其他組也是XDD希望下次可以好好認真做哈哈！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/aini.png\" width = \"180\" height = \"180\" alt=\"aini\" align=right />\n",
    "\n",
    "### 艾霓\n",
    "　　超可惜的啊啊啊啊啊啊啊啊啊啊啊啊啊啊啊啊啊啊啊啊啊啊啊啊啊啊啊<br />\n",
    "　　當初真的把希望都放在Bert上了哈哈哈，結果電腦完全跑不起來XD，禮拜一還拿者兩個記憶體跑去聽驛站，真的很好笑！<br />\n",
    "　　這次的競賽真的是在一片混亂之中度過的，覺得真的還有很多可以調整得更好的地方，可能自己更需要學習的是如何在有限的時間裡把事情都完成吧，像是這次的競賽到了最後一天才又要回來做原本的model，每個模型train一次就要花十幾分鐘甚至幾十分鐘，真的會來不及調整，結果就是明明知道還能做得更好卻也只能眼睜睜看著時間一分一秒過去，有點小失落。<br />\n",
    "　　在這個模型裡面，最有趣的部分其實是在CountVectorize的地方，那裡面不管是詞幹提取或是詞形還原，通通都一次一起做了，我覺得這樣的方法真的很酷！這個部分其實來自github，我在上面看到了roytsai用過這樣的做法(https://github.com/Roytsai27/Rating-Prediction-from-Movie-Review-) ，覺得很厲害就拿到了這裡使用xD，至於tf-idf因為在這個資料上表現得不好所以沒有採用。<br />\n",
    "　　再接再厲！！"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
